{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Note: Due to the large data size and model weights, we have uploaded DeepMReye related files (data, model weights, model history, results) to an external storage, which can be accessed [here](https://drive.google.com/drive/folders/1v8Et8A2n2PrOwLj3JJYkjh6VXxKMqX5C?usp=drive_link).\n",
    "\n",
    "sys.path.append('/DATA/publish/mocet/analysis/scripts')\n",
    "from utils.base import get_minecraft_subjects, get_project_directory, get_configs\n",
    "\n",
    "subject_pool = get_minecraft_subjects()\n",
    "project_dir = get_project_directory()\n",
    "configs = get_configs()\n",
    "\n",
    "valid_data = pickle.load(open('../../data/valid_data_list.pkl', 'rb'))\n",
    "valid_keys = list(valid_data.keys())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deepmreye import analyse, architecture, preprocess, train\n",
    "from deepmreye.util import data_generator, model_opts, util\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "experiment_folder = \"/DATA/publish/mocet/analysis/data/DeepMReye\"\n",
    "functional_data = os.path.join(\"/DATA\", \"Minecraft_old\", \"_DATA\", \"fMRI\")\n",
    "processed_data = os.path.join(experiment_folder, \"processed_data/\")\n",
    "model_weights = os.path.join(experiment_folder, \"model_weights/\")\n",
    "results = os.path.join(experiment_folder, \"results/\")\n",
    "\n",
    "if not os.path.exists(processed_data):\n",
    "    os.makedirs(processed_data)\n",
    "if not os.path.exists(model_weights):\n",
    "    os.makedirs(model_weights)\n",
    "if not os.path.exists(results):\n",
    "    os.makedirs(results)\n",
    "\n",
    "task = 'task-mcHERDING'\n",
    "space = 'space-MNI152NLin2009cAsym'\n",
    "prefix = 'desc-preproc_bold.nii.gz'\n",
    "\n",
    "eyemask_small, eyemask_big, dme_template, mask, x_edges, y_edges, z_edges, = preprocess.get_masks()\n",
    "subjects = subject_pool.keys()\n",
    "for subject in subjects:\n",
    "    sessions = subject_pool[subject].keys()\n",
    "    for session in sessions:\n",
    "        runs = subject_pool[subject][session]\n",
    "        root = f'{project_dir}/data/eyetracking/{subject}/{session}'\n",
    "        for r in runs:\n",
    "            run = f'run-{r}'\n",
    "            np.random.seed(0)\n",
    "            key = (subject, session, task, run)\n",
    "            if key in valid_keys:\n",
    "                print(subject, session, task, run)\n",
    "                fp_func = os.path.join(functional_data, subject, session,\n",
    "                                       f'{subject}_{session}_{task}_{run}_{space}_{prefix}')\n",
    "                if os.path.exists(os.path.join(functional_data, subject, session,\n",
    "                                               f'mask_{subject}_{session}_{task}_{run}_{space}_desc-preproc_bold.p')):\n",
    "                    print('Already preprocessed')\n",
    "                else:\n",
    "                    preprocess.run_participant(\n",
    "                        fp_func,\n",
    "                        dme_template,\n",
    "                        eyemask_big,\n",
    "                        eyemask_small,\n",
    "                        x_edges,\n",
    "                        y_edges,\n",
    "                        z_edges,\n",
    "                    )"
   ],
   "id": "9d2c3ad00e04490f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "calibration_points = np.array([[200-800, 166-500], [200-800, 500-500], [200-800, 833-500],\n",
    "                               [600-800, 166-500], [600-800, 500-500], [600-800, 833-500],\n",
    "                               [1000-800, 166-500], [1000-800, 500-500], [1000-800, 833-500],\n",
    "                               [1400-800, 166-500], [1400-800, 500-500], [1400-800, 833-500]]) * (1/78.0487)\n",
    "calibration_points = np.tile(calibration_points, [2,1])\n",
    "calibration_order = np.array([4,11,6,2,7,0,10,5,9,8,1,3]*2)\n",
    "\n",
    "subjects = subject_pool.keys()\n",
    "for subject in subjects:\n",
    "    sessions = subject_pool[subject].keys()\n",
    "    for session in sessions:\n",
    "        runs = subject_pool[subject][session]\n",
    "        for r in runs:\n",
    "            run = f'run-{r}'\n",
    "            np.random.seed(0)\n",
    "            key = (subject, session, task, run)\n",
    "            if key in valid_keys:\n",
    "                print(subject, session, task, run)\n",
    "                participant_train_data = []\n",
    "                participant_train_labels = []\n",
    "                participant_train_ids = []\n",
    "\n",
    "                participant_test_data = []\n",
    "                participant_test_labels = []\n",
    "                participant_test_ids = []\n",
    "\n",
    "                # Load mask and normalize it\n",
    "                # 216 TRs\n",
    "                this_mask = os.path.join(functional_data, subject, session,\n",
    "                                         f'mask_{subject}_{session}_{task}_{run}_{space}_desc-preproc_bold.p')\n",
    "                this_mask = pickle.load(open(this_mask, \"rb\"))\n",
    "                this_mask = preprocess.normalize_img(this_mask)\n",
    "                train_mask = np.copy(this_mask[...,1:25])\n",
    "                test_mask = np.copy(this_mask)\n",
    "\n",
    "                # Load labels (in visual angle)\n",
    "                #this_label = preprocess.load_label(gaze_data, label_type=\"calibration_run\")\n",
    "                train_label = calibration_points[calibration_order, :]\n",
    "                train_label = train_label[:, np.newaxis, :]\n",
    "                train_label = np.repeat(train_label, 10, axis=1)\n",
    "                test_label = np.zeros((510,10,2))\n",
    "\n",
    "                # Store across runs\n",
    "                participant_train_data.append(train_mask)\n",
    "                participant_train_labels.append(train_label)\n",
    "                participant_train_ids.append(([subject] * train_label.shape[0],[0] * train_label.shape[0]))\n",
    "\n",
    "                # Store across run\n",
    "                participant_test_data.append(test_mask)\n",
    "                participant_test_labels.append(test_label)\n",
    "                participant_test_ids.append(([subject] * test_label.shape[0],[0] * test_label.shape[0]))\n",
    "\n",
    "                # Save participant file\n",
    "                preprocess.save_data(\n",
    "                    subject + f'_{session}_{run}_train',\n",
    "                    participant_train_data,\n",
    "                    participant_train_labels,\n",
    "                    participant_train_ids,\n",
    "                    processed_data,\n",
    "                    center_labels=False,\n",
    "                )\n",
    "                preprocess.save_data(\n",
    "                    subject + f'_{session}_{run}_test',\n",
    "                    participant_test_data,\n",
    "                    participant_test_labels,\n",
    "                    participant_test_ids,\n",
    "                    processed_data,\n",
    "                    center_labels=False,\n",
    "                )\n"
   ],
   "id": "f2fce4e2f68c0d73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# From scratch\n",
    "\n",
    "import os\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class LossHistoryCallback(Callback):\n",
    "  def __init__(self, filename):\n",
    "    super().__init__()\n",
    "    self.filename = filename\n",
    "    with open(self.filename, \"w\") as f:\n",
    "      f.write(\"Epoch,Training Loss\\n\")\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    training_loss = logs.get('loss', 'NA')\n",
    "    with open(self.filename, \"a\") as f:\n",
    "      f.write(f\"{epoch+1},{training_loss}\\n\")\n",
    "\n",
    "# Training\n",
    "subjects = subject_pool.keys()\n",
    "for subject in subjects:\n",
    "    sessions = subject_pool[subject].keys()\n",
    "    for session in sessions:\n",
    "        runs = subject_pool[subject][session]\n",
    "        for r in runs:\n",
    "            run = f'run-{r}'\n",
    "            np.random.seed(0)\n",
    "            key = (subject, session, task, run)\n",
    "            if key in valid_keys:\n",
    "                print(subject, session, task, run)\n",
    "\n",
    "                data = np.load(f'{processed_data}/{subject}_{session}_{run}_train.npz', mmap_mode='r')\n",
    "                X = [data['data_' + str(b)] for b in range(24)]\n",
    "                y = [data['label_' + str(b)] for b in range(24)]\n",
    "                X = np.array(X)[..., np.newaxis]\n",
    "                y = np.array(y)\n",
    "                opts = model_opts.get_opts()\n",
    "\n",
    "                opts[\"epochs\"] = 120\n",
    "                opts[\"gaussian_noise\"] = 1.0\n",
    "                lr_sched = util.step_decay_schedule(initial_lr=opts['lr'], decay_factor=0.95, num_epochs=opts[\"epochs\"])\n",
    "\n",
    "                loss_history_callback = LossHistoryCallback(filename=f'{model_weights}/{subject}_{session}_{run}_loss_scratch.txt')\n",
    "                model, model_inference = architecture.create_standard_model(X.shape[1::], opts)\n",
    "                model.fit([X, y],\n",
    "                          epochs=opts[\"epochs\"],\n",
    "                          callbacks=[lr_sched, loss_history_callback],\n",
    "                          use_multiprocessing=True,\n",
    "                          workers=36)\n",
    "                model_inference.save_weights(f'{model_weights}/{subject}_{session}_{run}_model_scratch.h5')\n",
    "\n",
    "# Prediction\n",
    "for subject in subjects:\n",
    "    sessions = subject_pool[subject].keys()\n",
    "    for session in sessions:\n",
    "        runs = subject_pool[subject][session]\n",
    "        for r in runs:\n",
    "            run = f'run-{r}'\n",
    "            np.random.seed(0)\n",
    "            key = (subject, session, task, run)\n",
    "            if key in valid_keys:\n",
    "                print(subject, session, task, run)\n",
    "                test_data = np.load(f'{processed_data}/{subject}_{session}_{run}_test.npz', mmap_mode='r')\n",
    "\n",
    "                test_X = [test_data['data_' + str(b)] for b in range(510)]\n",
    "                test_y = [test_data['label_' + str(b)] for b in range(510)]\n",
    "                test_X = np.array(test_X)[..., np.newaxis]\n",
    "                test_y = np.array(test_y)\n",
    "\n",
    "                opts = model_opts.get_opts()\n",
    "                _, model_inference = architecture.create_standard_model(test_X.shape[1::], opts)\n",
    "                model_inference.load_weights(f'{model_weights}/{subject}_{session}_{run}_model_scratch.h5')\n",
    "                (pred_y, euc_pred) = model_inference.predict(test_X)\n",
    "\n",
    "                pred_y = np.median(pred_y, axis=1) #Original DeepMReye evaluation funcation used np.median\n",
    "                np.save(f'{results}/{subject}_{session}_{run}_pred_scratch.npy', pred_y)"
   ],
   "id": "e5e7554c4230d2e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Finetuned\n",
    "\n",
    "subjects = subject_pool.keys()\n",
    "for subject in subjects:\n",
    "    sessions = subject_pool[subject].keys()\n",
    "    for session in sessions:\n",
    "        runs = subject_pool[subject][session]\n",
    "        for r in runs:\n",
    "            run = f'run-{r}'\n",
    "            np.random.seed(0)\n",
    "            key = (subject, session, task, run)\n",
    "            if key in valid_keys:\n",
    "                print(subject, session, task, run)\n",
    "\n",
    "                data = np.load(f'{processed_data}/{subject}_{session}_{run}_train.npz', mmap_mode='r')\n",
    "                X = [data['data_' + str(b)] for b in range(24)]\n",
    "                y = [data['label_' + str(b)] for b in range(24)]\n",
    "                X = np.array(X)[..., np.newaxis]\n",
    "                y = np.array(y)\n",
    "                opts = model_opts.get_opts()\n",
    "\n",
    "                opts[\"epochs\"] = 120\n",
    "                opts[\"gaussian_noise\"] = 1.0\n",
    "                lr_sched = util.step_decay_schedule(initial_lr=opts['lr'], decay_factor=0.95, num_epochs=opts[\"epochs\"])\n",
    "\n",
    "                loss_history_callback = LossHistoryCallback(filename=f'{model_weights}/{subject}_{session}_{run}_loss_finetuned.txt')\n",
    "                model, model_inference = architecture.create_standard_model(X.shape[1::], opts)\n",
    "                model.load_weights(f'{model_weights}/pretrained_weight_datasets_1to6.h5')\n",
    "                model.fit([X, y],\n",
    "                          epochs=opts[\"epochs\"],\n",
    "                          callbacks=[lr_sched, loss_history_callback],\n",
    "                          use_multiprocessing=True,\n",
    "                          workers=36,\n",
    "                          )\n",
    "                model_inference.save_weights(f'{model_weights}/{subject}_{session}_{run}_model_finetuned.h5')\n",
    "\n",
    "# Prediction\n",
    "for subject in subjects:\n",
    "    sessions = subject_pool[subject].keys()\n",
    "    for session in sessions:\n",
    "        runs = subject_pool[subject][session]\n",
    "        for r in runs:\n",
    "            run = f'run-{r}'\n",
    "            np.random.seed(0)\n",
    "            key = (subject, session, task, run)\n",
    "            if key in valid_keys:\n",
    "                print(subject, session, task, run)\n",
    "                test_data = np.load(f'{processed_data}/{subject}_{session}_{run}_test.npz', mmap_mode='r')\n",
    "\n",
    "                test_X = [test_data['data_' + str(b)] for b in range(510)]\n",
    "                test_y = [test_data['label_' + str(b)] for b in range(510)]\n",
    "                test_X = np.array(test_X)[..., np.newaxis]\n",
    "                test_y = np.array(test_y)\n",
    "\n",
    "                opts = model_opts.get_opts()\n",
    "                _, model_inference = architecture.create_standard_model(test_X.shape[1::], opts)\n",
    "                model_inference.load_weights(f'{model_weights}/{subject}_{session}_{run}_model_finetuned.h5')\n",
    "                (pred_y, euc_pred) = model_inference.predict(test_X)\n",
    "\n",
    "                pred_y = np.median(pred_y, axis=1) #Original DeepMReye evaluation funcation used np.median\n",
    "                np.save(f'{results}/{subject}_{session}_{run}_pred_finetuned.npy', pred_y)"
   ],
   "id": "57d3a45e77a22eb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pretrained\n",
    "\n",
    "# Prediction\n",
    "for subject in subjects:\n",
    "    sessions = subject_pool[subject].keys()\n",
    "    for session in sessions:\n",
    "        runs = subject_pool[subject][session]\n",
    "        for r in runs:\n",
    "            run = f'run-{r}'\n",
    "            np.random.seed(0)\n",
    "            key = (subject, session, task, run)\n",
    "            if key in valid_keys:\n",
    "                print(subject, session, task, run)\n",
    "                test_data = np.load(f'{processed_data}/{subject}_{session}_{run}_test.npz', mmap_mode='r')\n",
    "\n",
    "                test_X = [test_data['data_' + str(b)] for b in range(510)]\n",
    "                test_y = [test_data['label_' + str(b)] for b in range(510)]\n",
    "                test_X = np.array(test_X)[..., np.newaxis]\n",
    "                test_y = np.array(test_y)\n",
    "\n",
    "                opts = model_opts.get_opts()\n",
    "                _, model_inference = architecture.create_standard_model(test_X.shape[1::], opts)\n",
    "                model_inference.load_weights(f'{model_weights}/pretrained_weight_datasets_1to6.h5')\n",
    "                (pred_y, euc_pred) = model_inference.predict(test_X)\n",
    "                pred_y = np.median(pred_y, axis=1) #Original DeepMReye evaluation funcation used np.median\n",
    "                np.save(f'{results}/{subject}_{session}_{run}_pred_pretrained.npy', pred_y)"
   ],
   "id": "7535d59e35ced66f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a4e05165d6d0049",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
